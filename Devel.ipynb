{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000, test=1000\n",
      "Photos: train=6000, test=1000\n"
     ]
    }
   ],
   "source": [
    "from CapGenerator import load_data as ld\n",
    "\n",
    "data = ld.prepare_dataset('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_descriptions = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-782ec568ef4b>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-782ec568ef4b>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    for key Pin descriptions.keys():\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from CapGenerator import generate_model as gen\n",
    "\n",
    "\n",
    "# convert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key Pin descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "        return all_desc\n",
    "\n",
    "    # fit a tokenizer given caption descriptions\n",
    "    def create_tokenizer(descriptions):\n",
    "        lines = to_lines(descriptions)\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(lines)\n",
    "        return tokenizer\n",
    "\n",
    "    # calculate the length of the description with the most words\n",
    "    def max_length(descriptions):\n",
    "        lines = to_lines(descriptions)\n",
    "        return max(len(d.split()) for d in lines)\n",
    "\n",
    "    # create sequences of images, input sequences and output words for an image\n",
    "    def create_sequences(tokenizer, max_length, desc_list, photo):\n",
    "        X1, X2, y = list(), list(), list()\n",
    "        # walk through each description for the image\n",
    "        for desc in desc_list:\n",
    "            # encode the sequence\n",
    "            seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "            # split one sequence into multiple X,y pairs\n",
    "            for i in range(1, len(seq)):\n",
    "             s\n",
    "            # split into input and output pair\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                # pad input sequence\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                # encode output sequence\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                # store\n",
    "                X1.append(photo)\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "                return np.array(X1), np.array(X2), np.array(y)\n",
    "\n",
    "            # data generator, intended to be used in a call to model.fit_generator()\n",
    "            def data_generator(descriptions, photos, tokenizer, max_length):\n",
    "                # loop for ever over images\n",
    "                while 1:\n",
    "                    for key, desc_list in descriptions.items():\n",
    "                        # retrieve the photo feature\n",
    "                        photo = photos[key][0]\n",
    "                        print('Photo:')\n",
    "                        print(photo.shape)\n",
    "                        in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo)\n",
    "                        yield [[in_img, in_seq], out_word]\n",
    "\n",
    "                        # prepare tokenizer\n",
    "                        tokenizer = create_tokenizer(train_descriptions)\n",
    "                        vocab_size = len(tokenizer.word_index) + 1\n",
    "                        print('Vocabulary Size: %d' % vocab_size)\n",
    "\n",
    "                        # determine the maximum sequence length\n",
    "                        max_length = max_length(train_descriptions)\n",
    "                        print('Description Length: %d' % max_length)\n",
    "\n",
    "                        # test the data generator\n",
    "                        generator = gen.data_generator(train_descriptions, train_features, tokenizer, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs = next(generator)\n",
    "print(inputs[0].shape)\n",
    "print(inputs[1].shape)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import concatenate\n",
    "from keras.layers.merge import add\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "\n",
    "\n",
    "def define_model(vocab_size, max_length):\n",
    "    # feature extractor (encoder)\n",
    "    inputs1 = Input(shape=(4096, ))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(EMBEDDING_DIM, activation='relu')(fe1)\n",
    "    fe3 = RepeatVector(max_length)(fe2)\n",
    "\n",
    "    # embedding\n",
    "    inputs2 = Input(shape=(max_length, ))\n",
    "    emb2 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    emb3 = LSTM(256, return_sequences=True)(emb2)\n",
    "    emb4 = TimeDistributed(Dense(EMBEDDING_DIM, activation='relu'))(emb3)\n",
    "\n",
    "    # merge inputs\n",
    "    merged = concatenate([fe3, emb4])\n",
    "    # language model (decoder)\n",
    "    lm2 = LSTM(1000)(merged)\n",
    "    #lm3 = Dense(500, activation='relu')(lm2)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(lm2)\n",
    "\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    plot_model(model, show_shapes=True, to_file='plot.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4096)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 34, 256)      51200       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          524416      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 34, 256)      525312      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 34, 128)      0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 34, 128)      32896       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 34, 256)      0           repeat_vector_1[0][0]            \n",
      "                                                                 time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 1000)         5028000     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 200)          200200      lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 6,362,024\n",
      "Trainable params: 6,362,024\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = define_model(200, 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=True, input_shape = (224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model, show_shapes=True, show_layer_names=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in train_descriptions.items():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "    t = Tokenizer()  # all without .\n",
    "    text = \"<start> Tomorrow will be cold. <end>\"\n",
    "    text = text.replace(\".\", \" .\")\n",
    "    t.fit_on_texts([text])\n",
    "    print(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from each photo in the directory\n",
    "def extract_features(filename):\n",
    "    # load the model\n",
    "    model = VGG16()\n",
    "    # re-structure the model\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    # load the photo\n",
    "    image = load_img(filename, target_size=(224, 224))\n",
    "    # convert the image pixels to a numpy array\n",
    "    image = img_to_array(image)\n",
    "    # reshape data for the model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # prepare the image for the VGG model\n",
    "    image = preprocess_input(image)\n",
    "    # get features\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    return feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda",
   "language": "python",
   "name": "anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
